{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "76OyEnx-EnAd"
      ],
      "authorship_tag": "ABX9TyOWy4mRBpD+AG7kJNl++flT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tianananana/MAML-Naive-Walkthrough/blob/main/v2_MAML_P%26P_(Autograd).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSWqsVh2dM-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pdb\n",
        "import copy\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HS5MNVDHb5O4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes"
      ],
      "metadata": {
        "id": "yjPnGS494bmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleNet():\n",
        "  def __init__(self):\n",
        "    theta = torch.tensor([1.], requires_grad=True) # we will need the gradients of meta model weights.\n",
        "    # theta.retain_grad()\n",
        "    self.weight = theta # set initial weight to 1\n",
        "    self.retain_gradient()\n",
        "  \n",
        "  def __call__(self, x, phij=None):\n",
        "    if phij is None:\n",
        "      return self.weight * x\n",
        "    else:\n",
        "      # manally pass in task-specific weights\n",
        "      return phij * x\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    self.weight.grad = None # setting gradients to None deletes all previous computational tree.\n",
        "\n",
        "  def retain_gradient(self):\n",
        "    self.weight.retain_grad()"
      ],
      "metadata": {
        "id": "2uaXBGeab2v_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def L1loss(self, pred, targ):\n",
        "    loss = torch.sum((pred - targ)**2)\n",
        "    return loss\n",
        "\n",
        "  def __call__(self, pred, targ):\n",
        "    return self.L1loss(pred, targ)"
      ],
      "metadata": {
        "id": "8wwkMW7PB1Um"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimSGD():\n",
        "  def __init__(self, net, lr):\n",
        "    self.net = net\n",
        "    self.lr = lr\n",
        "\n",
        "  def _SGD(self):\n",
        "    # SGD formula\n",
        "    self.net.weight = self.net.weight - self.lr * self.net.weight.grad\n",
        "    self.net.retain_gradient()\n",
        "    print(self.net.weight)\n",
        "\n",
        "\n",
        "  def step(self):\n",
        "    self._SGD()"
      ],
      "metadata": {
        "id": "kwCEdzzlBgcX"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v1 New pipeline with SingleNet, Loss, and Optim class"
      ],
      "metadata": {
        "id": "IouYzdhfwQmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Inner Trainer: compute task-specific weights (phi_j)"
      ],
      "metadata": {
        "id": "V3KPcyQAxL38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InnerTrainer:\n",
        "  def __init__(self, net, alpha, criterion):\n",
        "    self.net = net\n",
        "    self.alpha = alpha\n",
        "    self.loss = criterion\n",
        "    self.tmp_weight = None # for k step inner update\n",
        "\n",
        "  def _one_inner_epoch(self, x, y):\n",
        "    if self.tmp_weight is None:\n",
        "      # use theta (meta-model weights)\n",
        "      pred = self.net(x) \n",
        "      curr_weight = self.net.weight\n",
        "    else:\n",
        "      # use intermmediate steps\n",
        "      pred = self.net(x, self.tmp_weight) \n",
        "      curr_weight = self.tmp_weight\n",
        "\n",
        "    loss = self.loss(pred, y)\n",
        "    \n",
        "    # computing phi_j\n",
        "    gradient = torch.autograd.grad(loss, curr_weight, create_graph=True)[0]\n",
        "    print(f\"INNER GRADIENT: {gradient.item()}\")\n",
        "    self.tmp_weight = curr_weight - self.alpha * gradient\n",
        "    print(f\"PHI_J: {self.tmp_weight.item()}\")\n",
        "\n",
        "    return self.tmp_weight, gradient\n",
        "\n",
        "\n",
        "  def k_inner_epoch(self, x, y, k):\n",
        "    \"\"\" Returns phi_j \"\"\"\n",
        "    # Outputs both phi_j and phi_j gradients for logging.\n",
        "    final_phi_j = None\n",
        "    final_grad = None\n",
        "    for i in range(k):\n",
        "      final_phi_j, final_grad = self._one_inner_epoch(x, y)\n",
        "\n",
        "    return final_phi_j, final_grad"
      ],
      "metadata": {
        "id": "8Y4y_e1zJLVQ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Meta Trainer: update meta weight (theta)"
      ],
      "metadata": {
        "id": "b422xQAz28O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetaTrainer:\n",
        "  def __init__(self, net, alpha, beta, criterion, k=1):\n",
        "    self.net = net\n",
        "    self.loss = criterion\n",
        "    self.k = k\n",
        "    self.alpha = alpha\n",
        "    self.opt = OptimSGD(self.net, beta)\n",
        "\n",
        "  def one_epoch(self, datasets):\n",
        "    # loss_all = torch.tensor([0.])\n",
        "    self.net.zero_grad()\n",
        "\n",
        "    # Initialize dicts per dataset\n",
        "    info['InnerGrad'], info['PhiJ'] = {}, {}\n",
        "    info['L_q'], info['OuterGrad(Accum)'] = {}, {}\n",
        "\n",
        "    for i in range(len(datasets)):\n",
        "      # iterate over each dataset (D1, D2)\n",
        "      print(f\"RUNNING INNER LOOP OF D{i+1}\")\n",
        "      sx, sy = datasets[i]['support'][:, 0], datasets[i]['support'][:, 1]\n",
        "      qx, qy = datasets[i]['query'][:, 0], datasets[i]['query'][:, 1]\n",
        "      \n",
        "      innerTrainer = InnerTrainer(net=self.net, alpha=self.alpha, criterion=self.loss)\n",
        "\n",
        "      # Initialize list for each dataset\n",
        "      phi_j, phi_j_grad = innerTrainer.k_inner_epoch(sx, sy, self.k)\n",
        "      info['PhiJ'][f'D{i+1}'], info['InnerGrad'][f'D{i+1}'] = phi_j.item(), phi_j_grad.item()\n",
        "        ## Check whether 2nd outer epoch calls tmp_weights on 1st inner training or use net.weight.\n",
        "\n",
        "      pred = self.net(qx, phi_j)\n",
        "      loss = self.loss(pred, qy)\n",
        "      print(f\"L_Q OF D{i+1}: {loss}\")\n",
        "      info['L_q'][f'D{i+1}'] = loss.item()\n",
        "\n",
        "      loss.backward() # take gradients for each dataset separately\n",
        "      print(f\"ACCUM GRAD OF L_Q OF D{i+1}: {self.net.weight.grad}\") #v2 TODO: think of a way to remove comp tree aft accumulating grads\n",
        "      info['OuterGrad(Accum)'][f'D{i+1}'] = self.net.weight.grad.item()\n",
        "\n",
        "    # update meta weights (theta)\n",
        "    self.opt.step()\n",
        "\n",
        "    print(f\"NEW META WEIGHT {self.net.weight.item()}\")\n",
        "    info['NewWeight'] = self.net.weight.item()"
      ],
      "metadata": {
        "id": "ndDsYM0Bu6vh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "beta = 0.5\n",
        "net = SingleNet()\n",
        "L1 = Loss()"
      ],
      "metadata": {
        "id": "HGQ0LV6AwPyJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "J6955v1kbxmP"
      },
      "outputs": [],
      "source": [
        "# We define 2 datasets for our case. \n",
        "D1 = {'query': torch.tensor([(1, 2)], dtype=torch.int64), 'support': torch.tensor([(2, 4), (3, 1)])} # (x, y) pairs for query (Q1) and support (S1) set.\n",
        "D2 = {'query': torch.tensor([(4, 1)], dtype=torch.int64), 'support': torch.tensor([(5, 3), (6, 0)])}\n",
        "D_all = [D1, D2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = MetaTrainer(net=net, alpha=alpha, beta=beta, criterion=L1, k=1)\n",
        "# initialize dict to store in json file\n",
        "log = {}\n",
        "log_dir = './log'\n",
        "\n",
        "for i in range(3):\n",
        "  print(f\"\\nEPOCH {i}\")\n",
        "  info = {}\n",
        "  info['Epoch'] = i\n",
        "  trainer.one_epoch(D_all)\n",
        "\n",
        "  # append to current json file. Save each epoch logs once.\n",
        "  with open('./log.json', 'a') as log:\n",
        "    json.dump(info, log)\n",
        "    log.write('\\n')\n"
      ],
      "metadata": {
        "id": "sXbPP9svxEzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Read json output file into pandas DataFrame \"\"\"\n",
        "df = pd.read_json('/content/log.json', lines=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "uMe2QHlIbGBr",
        "outputId": "7252aba7-9f44-4f3f-8192-671a4d6046c1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Epoch                                    InnerGrad  \\\n",
              "0      0                      {'D1': 4.0, 'D2': 92.0}   \n",
              "1      1  {'D1': -39424.48046875, 'D2': -184918.5625}   \n",
              "2      2        {'D1': 79156624.0, 'D2': 371427328.0}   \n",
              "\n",
              "                                                PhiJ  \\\n",
              "0  {'D1': 0.600000023841857, 'D2': -8.19999980926...   \n",
              "1   {'D1': 2426.968017578125, 'D2': 16976.376953125}   \n",
              "2              {'D1': -4871176.0, 'D2': -34098244.0}   \n",
              "\n",
              "                                                 L_q  \\\n",
              "0  {'D1': 1.959999918937683, 'D2': 1142.43994140625}   \n",
              "1              {'D1': 5880470.0, 'D2': 4611022336.0}   \n",
              "2  {'D1': 23728375201792.0, 'D2': 1.8603043104751...   \n",
              "\n",
              "                                   OuterGrad(Accum)     NewWeight  \n",
              "0  {'D1': 4.479999542236328, 'D2': 3032.9599609375} -1.515480e+03  \n",
              "1       {'D1': -7759.89794921875, 'D2': -6092004.0}  3.044486e+06  \n",
              "2           {'D1': 15587772.0, 'D2': 12236398592.0} -6.115155e+09  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3b53ed8-13f5-4f52-b5ab-c235ad5d27ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>InnerGrad</th>\n",
              "      <th>PhiJ</th>\n",
              "      <th>L_q</th>\n",
              "      <th>OuterGrad(Accum)</th>\n",
              "      <th>NewWeight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>{'D1': 4.0, 'D2': 92.0}</td>\n",
              "      <td>{'D1': 0.600000023841857, 'D2': -8.19999980926...</td>\n",
              "      <td>{'D1': 1.959999918937683, 'D2': 1142.43994140625}</td>\n",
              "      <td>{'D1': 4.479999542236328, 'D2': 3032.9599609375}</td>\n",
              "      <td>-1.515480e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>{'D1': -39424.48046875, 'D2': -184918.5625}</td>\n",
              "      <td>{'D1': 2426.968017578125, 'D2': 16976.376953125}</td>\n",
              "      <td>{'D1': 5880470.0, 'D2': 4611022336.0}</td>\n",
              "      <td>{'D1': -7759.89794921875, 'D2': -6092004.0}</td>\n",
              "      <td>3.044486e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>{'D1': 79156624.0, 'D2': 371427328.0}</td>\n",
              "      <td>{'D1': -4871176.0, 'D2': -34098244.0}</td>\n",
              "      <td>{'D1': 23728375201792.0, 'D2': 1.8603043104751...</td>\n",
              "      <td>{'D1': 15587772.0, 'D2': 12236398592.0}</td>\n",
              "      <td>-6.115155e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3b53ed8-13f5-4f52-b5ab-c235ad5d27ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3b53ed8-13f5-4f52-b5ab-c235ad5d27ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3b53ed8-13f5-4f52-b5ab-c235ad5d27ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v0 Old code (Loss functions, task-specific functions, meta functions)"
      ],
      "metadata": {
        "id": "76OyEnx-EnAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" LOSS FUNCTIONS \"\"\"\n",
        "def loss(weight, dataset, mode='train'):\n",
        "  # Eqn 5: l1 loss\n",
        "  if mode == 'train':\n",
        "    data = dataset['support']\n",
        "  if mode == 'test':\n",
        "    data = dataset['query']\n",
        "    # HK: loss function need to pass in only pred and target.\n",
        "  return torch.sum((data[:, 1] - weight * data[:, 0])**2)"
      ],
      "metadata": {
        "id": "4xp61SkCnSZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" TASK SPECIFIC FUNCTIONS \"\"\"\n",
        "def inner_gradient(weight, dataset, mode='train'):\n",
        "  # Eqn 6\n",
        "  if mode == 'train':\n",
        "    data = dataset['support']\n",
        "\n",
        "  if mode == 'test':\n",
        "    data = dataset['query']\n",
        "\n",
        "  # HK: Only pass in dataset\n",
        "  # add in loss variable\n",
        "  \n",
        "\n",
        "  # Use torch.autograd.grad\n",
        "  return grad(loss(weight, dataset, mode=mode), weight, create_graph=True)[0]\n",
        "  # return grad(loss(weight, dataset, mode=mode), weight)[0]\n",
        "  \n",
        "def inner_weight(weight, dataset, alpha=0.1, mode='train'):\n",
        "  # Eqn 7\n",
        "  task_specific_grads = inner_gradient(weight, dataset, mode=mode)\n",
        "  return weight - alpha * task_specific_grads"
      ],
      "metadata": {
        "id": "ogeArnOnnLkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" META FUNCTIONS \"\"\"\n",
        "def meta_gradient_1(theta_0, dataset):\n",
        "  \"\"\" dag term: derivative of phi_j wrt theta_0 \"\"\"\n",
        "  phi_j_test = inner_weight(theta_0, dataset, mode='test')\n",
        "  # Use torch.autograd.grad\n",
        "  grad_wrt_theta_0 = grad(phi_j_test, theta_0)[0]\n",
        "  return grad_wrt_theta_0\n",
        "\n",
        "def meta_gradient_2(phi_j, dataset):\n",
        "  \"\"\" ddag term: derivative wrt phi_j \"\"\"\n",
        "  meta_loss = loss(phi_j, dataset, mode='test')\n",
        "  # Use torch.autograd.grad\n",
        "  grad_wrt_phi_j = grad(meta_loss, phi_j)[0]\n",
        "  return grad_wrt_phi_j\n",
        "\n",
        "\n",
        "def meta_gradient(theta_0, dataset):\n",
        "  \"\"\" Getting meta_gradient by applying chain rule \"\"\"\n",
        "  dag_term = meta_gradient_1(theta_0, dataset)\n",
        "  phi_j = inner_weight(theta_0, dataset, mode='train')\n",
        "  ddag_term = meta_gradient_2(phi_j, dataset)\n",
        "  # use the chain rule\n",
        "  return dag_term * ddag_term\n",
        "\n",
        "def meta_loss(theta_0, dataset, alpha=0.1, beta=0.5):\n",
        "  \"\"\" Computing the meta loss with meta gradients \"\"\"\n",
        "  total_grads = torch.tensor([0.])\n",
        "  for d in dataset:\n",
        "    total_grads += meta_gradient(theta_0, d)\n",
        "  return theta_0 - beta * total_grads\n"
      ],
      "metadata": {
        "id": "nvIzN41pb-Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = D1\n",
        "net = SingleNet()\n",
        "phi_j_test = inner_weight(net.weight, dataset, mode='test')\n",
        "phi_j_train = inner_weight(net.weight, dataset, mode='train')\n",
        "meta_loss = loss(phi_j_train, dataset, mode='test')\n",
        "print(meta_loss)\n",
        "meta_loss.backward()\n",
        "phi_j_train.backward()\n",
        "print(net.weight.grad)\n",
        "print(phi_j_train)\n",
        "print(meta_loss.grad_fn(phi_j_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM_3frhpU0Yc",
        "outputId": "2fed57ef-107a-48a1-ac4f-5ee7ccfdf388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9600, grad_fn=<SumBackward0>)\n",
            "tensor([-1.8000])\n",
            "tensor([0.6000], grad_fn=<SubBackward0>)\n",
            "tensor([0.6000], grad_fn=<ExpandBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZV_C7PtNevog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## UNIT TEST ##\n",
        "net = SingleNet()\n",
        "# Output task-specific gradient\n",
        "inner_grad1 = inner_gradient(net.weight, D1, mode='train')\n",
        "inner_grad2 = inner_gradient(net.weight, D2, mode='train')\n",
        "print(f\"{inner_grad1=}\")\n",
        "print(f\"{inner_grad2=}\")\n",
        "print(\"\")\n",
        "\n",
        "# Output task-specific weights\n",
        "w1 = inner_weight(net.weight, D1)\n",
        "w2 = inner_weight(net.weight, D2)\n",
        "print(f\"{w1=}\")\n",
        "print(f\"{w2=}\")\n",
        "print(\"\")\n",
        "\n",
        "# Output meta gradient\n",
        "meta_grad1 = meta_gradient(net.weight, D1)\n",
        "meta_grad2 = meta_gradient(net.weight, D2)\n",
        "print(f\"{meta_grad1}\")\n",
        "print(f\"{meta_grad2}\")\n",
        "print(\"\")\n",
        "\n",
        "# Output new meta weight\n",
        "theta_1 = meta_loss(net.weight, D_all)\n",
        "print(f\"{theta_1=}\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_m2ALHWRHsF",
        "outputId": "525915b2-1506-4617-a654-4db4d38c795f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inner_grad1=tensor([4.], grad_fn=<SumBackward1>)\n",
            "inner_grad2=tensor([92.], grad_fn=<SumBackward1>)\n",
            "\n",
            "w1=tensor([0.6000], grad_fn=<SubBackward0>)\n",
            "w2=tensor([-8.2000], grad_fn=<SubBackward0>)\n",
            "\n",
            "tensor([-2.2400])\n",
            "tensor([594.8800])\n",
            "\n",
            "theta_1=tensor([-295.3200], grad_fn=<SubBackward0>)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}