{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "76OyEnx-EnAd"
      ],
      "authorship_tag": "ABX9TyNgnDo6mwaqdacXW+LB7bcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tianananana/MAML-Naive-Walkthrough/blob/main/v1_MAML_P%26P_(Autograd).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V1 CHANGES (6/1/2023):\n",
        "TODO:\n",
        "- ET, LW, HK: Print compute tree. Take ref from here: https://github.com/szagoruyko/pytorchviz\n",
        "\n",
        "DONE:\n",
        "- add Loss class\n",
        "- add optimizer\n",
        "- figure out how to compute gradient in single step (no chain rule)\n",
        "\n"
      ],
      "metadata": {
        "id": "9FQLu3JWaAW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSDMec77POkX",
        "outputId": "aa7da618-b2c2-49cf-fad7-cd67abc9468c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchviz) (1.13.0+cu116)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchviz) (4.4.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4151 sha256=a7ae6fe815b25fbd361532ec0a8e9993e45e156c6368a8296ad6211f8ae25200\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/7d/1b/8306781244e42ede119edbb053bdcda1c1f424ca226165a417\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchviz import make_dot, make_dot_from_trace"
      ],
      "metadata": {
        "id": "HS5MNVDHb5O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_test = SingleNet()\n",
        "print(net_test(5))\n",
        "make_dot(net_test(5), params={\"weight\": net.weight})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "52x5lqLyPZMC",
        "outputId": "37bd18d1-5eab-42dd-f6cb-36a927d41303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.], grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fd1dac72280>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"109pt\" height=\"216pt\"\n viewBox=\"0.00 0.00 109.00 216.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 212)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-212 105,-212 105,4 -4,4\"/>\n<!-- 140541411557184 -->\n<g id=\"node1\" class=\"node\">\n<title>140541411557184</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140539295375952 -->\n<g id=\"node2\" class=\"node\">\n<title>140539295375952</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-86 6,-86 6,-67 95,-67 95,-86\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140539295375952&#45;&gt;140541411557184 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140539295375952&#45;&gt;140541411557184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-66.9688C50.5,-60.1289 50.5,-50.5621 50.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-41.3678 50.5,-31.3678 47.0001,-41.3678 54.0001,-41.3678\"/>\n</g>\n<!-- 140539295376144 -->\n<g id=\"node3\" class=\"node\">\n<title>140539295376144</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140539295376144&#45;&gt;140539295375952 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140539295376144&#45;&gt;140539295375952</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-121.9197C50.5,-114.9083 50.5,-105.1442 50.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-96.3408 50.5,-86.3408 47.0001,-96.3409 54.0001,-96.3408\"/>\n</g>\n<!-- 140539295459824 -->\n<g id=\"node4\" class=\"node\">\n<title>140539295459824</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-208 23.5,-208 23.5,-177 77.5,-177 77.5,-208\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140539295459824&#45;&gt;140539295376144 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140539295459824&#45;&gt;140539295376144</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-176.791C50.5,-169.0249 50.5,-159.5706 50.5,-151.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-151.0647 50.5,-141.0648 47.0001,-151.0648 54.0001,-151.0647\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes"
      ],
      "metadata": {
        "id": "yjPnGS494bmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleNet():\n",
        "  def __init__(self):\n",
        "    theta = torch.tensor([1.], requires_grad=True) # we will need the gradients of meta model weights.\n",
        "    theta.retain_grad()\n",
        "    self.weight = theta # set initial weight to 1\n",
        "    # self.weight.retain_grad()\n",
        "  \n",
        "  def __call__(self, x, phij=None):\n",
        "    if phij is None:\n",
        "      return self.weight * x\n",
        "    else:\n",
        "      # manally pass in task-specific weights\n",
        "      return phij * x\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    if self.weight.grad is None:\n",
        "      self.weight.grad = torch.tensor([0.])\n",
        "    self.weight.grad = self.weight.grad.zero_()"
      ],
      "metadata": {
        "id": "2uaXBGeab2v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def L1loss(self, pred, targ):\n",
        "    loss = torch.sum((pred - targ)**2)\n",
        "    return loss\n",
        "\n",
        "  def __call__(self, pred, targ):\n",
        "    return self.L1loss(pred, targ)"
      ],
      "metadata": {
        "id": "8wwkMW7PB1Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimSGD():\n",
        "  def __init__(self, net, lr):\n",
        "    self.net = net\n",
        "    self.lr = lr\n",
        "\n",
        "  def SGD(self):\n",
        "    # SGD formula\n",
        "    self.net.weight = self.net.weight - self.lr * self.net.weight.grad\n",
        "\n",
        "  def step(self):\n",
        "    self.SGD()"
      ],
      "metadata": {
        "id": "kwCEdzzlBgcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v1 New pipeline with SingleNet, Loss, and Optim class"
      ],
      "metadata": {
        "id": "IouYzdhfwQmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Inner Trainer: compute task-specific weights (phi_j)"
      ],
      "metadata": {
        "id": "V3KPcyQAxL38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InnerTrainer:\n",
        "  def __init__(self, net, lr, criterion):\n",
        "    self.net = net\n",
        "    self.alpha = alpha\n",
        "    self.loss = criterion\n",
        "\n",
        "  def one_inner_epoch(self, x, y):\n",
        "    \"\"\" Returns phi_j \"\"\"\n",
        "    self.net.zero_grad()\n",
        "    pred = self.net(x) # use theta (meta-model weights)\n",
        "    loss = self.loss(pred, y)\n",
        "    gradient = torch.autograd.grad(loss, self.net.weight, create_graph=True)[0]\n",
        "    print(f\"INNER GRADIENT: {gradient}\")\n",
        "    # computing phi_j\n",
        "    inner_weight = self.net.weight - self.alpha * gradient\n",
        "    print(f\"PHI_J: {inner_weight}\")\n",
        "    print(f\"PHI_J GRAD: {self.net.weight.grad}\")\n",
        "    return inner_weight"
      ],
      "metadata": {
        "id": "8Y4y_e1zJLVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Meta Trainer: update meta weight (theta)"
      ],
      "metadata": {
        "id": "b422xQAz28O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetaTrainer:\n",
        "  def __init__(self, net, alpha, beta, criterion):\n",
        "    self.net = net\n",
        "    self.loss = criterion\n",
        "    self.innerTrainer = InnerTrainer(net=net, lr=alpha, criterion=criterion)\n",
        "    self.opt = OptimSGD(self.net, beta)\n",
        "\n",
        "  def one_epoch(self, datasets):\n",
        "    loss_all = torch.tensor([0.])\n",
        "    self.net.zero_grad()\n",
        "    for i in range(len(datasets)):\n",
        "      # iterate over each dataset (D1, D2)\n",
        "      print(f\"RUNNING INNER LOOP OF D{i+1}\")\n",
        "      sx, sy = datasets[i]['support'][:, 0], datasets[i]['support'][:, 1]\n",
        "      qx, qy = datasets[i]['query'][:, 0], datasets[i]['query'][:, 1]\n",
        "      phi_j = self.innerTrainer.one_inner_epoch(sx, sy)\n",
        "      pred = self.net(qx, phi_j)\n",
        "      loss = self.loss(pred, qy)\n",
        "      print(f\"LOSS OVER QUERY: {loss}\")\n",
        "      loss_all += loss\n",
        "\n",
        "    loss_all.backward()\n",
        "\n",
        "    print(f\"BEF STEP: {self.net.weight.grad=}\")\n",
        "\n",
        "    # update meta weights (theta)\n",
        "    self.opt.step()\n",
        "\n",
        "    print(f\"AFT STEP: {self.net.weight.grad=}\")\n",
        "    print(f\"{self.net.weight=}\")"
      ],
      "metadata": {
        "id": "ndDsYM0Bu6vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "beta = 0.5\n",
        "net = SingleNet()\n",
        "L1 = Loss()"
      ],
      "metadata": {
        "id": "HGQ0LV6AwPyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6955v1kbxmP",
        "outputId": "385e7df4-21d0-4d37-f387-1a58a963b33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_D1 (support set): tensor([2, 3])\n",
            "y_D1 (support set): tensor([4, 1])\n"
          ]
        }
      ],
      "source": [
        "# We define 2 datasets for our case. \n",
        "D1 = {'query': torch.tensor([(1, 2)], dtype=torch.int64), 'support': torch.tensor([(2, 4), (3, 1)])} # (x, y) pairs for query (Q1) and support (S1) set.\n",
        "D2 = {'query': torch.tensor([(4, 1)], dtype=torch.int64), 'support': torch.tensor([(5, 3), (6, 0)])}\n",
        "D_all = [D1, D2]\n",
        "print(f\"x_D1 (support set): {D1['support'][:,0]}\")  # x\n",
        "print(f\"y_D1 (support set): {D1['support'][:,1]}\")  # y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = MetaTrainer(net=net, alpha=alpha, beta=beta, criterion=L1)\n",
        "trainer.one_epoch(D_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXbPP9svxEzI",
        "outputId": "e11abe2c-9344-4ec7-bea7-69de19de5a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING INNER LOOP OF D1\n",
            "INNER GRADIENT: tensor([4.], grad_fn=<SumBackward1>)\n",
            "PHI_J: tensor([0.6000], grad_fn=<SubBackward0>)\n",
            "PHI_J GRAD: tensor([0.])\n",
            "LOSS OVER QUERY: 1.959999918937683\n",
            "RUNNING INNER LOOP OF D2\n",
            "INNER GRADIENT: tensor([92.], grad_fn=<SumBackward1>)\n",
            "PHI_J: tensor([-8.2000], grad_fn=<SubBackward0>)\n",
            "PHI_J GRAD: tensor([0.])\n",
            "LOSS OVER QUERY: 1142.43994140625\n",
            "BEF STEP: self.net.weight.grad=tensor([3032.9600])\n",
            "AFT STEP: self.net.weight.grad=None\n",
            "self.net.weight=tensor([-1515.4800], grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-1be479516587>:29: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  print(f\"AFT STEP: {self.net.weight.grad=}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v0 Old code (Loss functions, task-specific functions, meta functions)"
      ],
      "metadata": {
        "id": "76OyEnx-EnAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" LOSS FUNCTIONS \"\"\"\n",
        "def loss(weight, dataset, mode='train'):\n",
        "  # Eqn 5: l1 loss\n",
        "  if mode == 'train':\n",
        "    data = dataset['support']\n",
        "  if mode == 'test':\n",
        "    data = dataset['query']\n",
        "    # HK: loss function need to pass in only pred and target.\n",
        "  return torch.sum((data[:, 1] - weight * data[:, 0])**2)"
      ],
      "metadata": {
        "id": "4xp61SkCnSZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" TASK SPECIFIC FUNCTIONS \"\"\"\n",
        "def inner_gradient(weight, dataset, mode='train'):\n",
        "  # Eqn 6\n",
        "  if mode == 'train':\n",
        "    data = dataset['support']\n",
        "\n",
        "  if mode == 'test':\n",
        "    data = dataset['query']\n",
        "\n",
        "  # HK: Only pass in dataset\n",
        "  # add in loss variable\n",
        "  \n",
        "\n",
        "  # Use torch.autograd.grad\n",
        "  return grad(loss(weight, dataset, mode=mode), weight, create_graph=True)[0]\n",
        "  # return grad(loss(weight, dataset, mode=mode), weight)[0]\n",
        "  \n",
        "def inner_weight(weight, dataset, alpha=0.1, mode='train'):\n",
        "  # Eqn 7\n",
        "  task_specific_grads = inner_gradient(weight, dataset, mode=mode)\n",
        "  return weight - alpha * task_specific_grads"
      ],
      "metadata": {
        "id": "ogeArnOnnLkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" META FUNCTIONS \"\"\"\n",
        "def meta_gradient_1(theta_0, dataset):\n",
        "  \"\"\" dag term: derivative of phi_j wrt theta_0 \"\"\"\n",
        "  phi_j_test = inner_weight(theta_0, dataset, mode='test')\n",
        "  # Use torch.autograd.grad\n",
        "  grad_wrt_theta_0 = grad(phi_j_test, theta_0)[0]\n",
        "  return grad_wrt_theta_0\n",
        "\n",
        "def meta_gradient_2(phi_j, dataset):\n",
        "  \"\"\" ddag term: derivative wrt phi_j \"\"\"\n",
        "  meta_loss = loss(phi_j, dataset, mode='test')\n",
        "  # Use torch.autograd.grad\n",
        "  grad_wrt_phi_j = grad(meta_loss, phi_j)[0]\n",
        "  return grad_wrt_phi_j\n",
        "\n",
        "\n",
        "def meta_gradient(theta_0, dataset):\n",
        "  \"\"\" Getting meta_gradient by applying chain rule \"\"\"\n",
        "  dag_term = meta_gradient_1(theta_0, dataset)\n",
        "  phi_j = inner_weight(theta_0, dataset, mode='train')\n",
        "  ddag_term = meta_gradient_2(phi_j, dataset)\n",
        "  # use the chain rule\n",
        "  return dag_term * ddag_term\n",
        "\n",
        "def meta_loss(theta_0, dataset, alpha=0.1, beta=0.5):\n",
        "  \"\"\" Computing the meta loss with meta gradients \"\"\"\n",
        "  total_grads = torch.tensor([0.])\n",
        "  for d in dataset:\n",
        "    total_grads += meta_gradient(theta_0, d)\n",
        "  return theta_0 - beta * total_grads\n"
      ],
      "metadata": {
        "id": "nvIzN41pb-Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = D1\n",
        "net = SingleNet()\n",
        "phi_j_test = inner_weight(net.weight, dataset, mode='test')\n",
        "phi_j_train = inner_weight(net.weight, dataset, mode='train')\n",
        "meta_loss = loss(phi_j_train, dataset, mode='test')\n",
        "print(meta_loss)\n",
        "meta_loss.backward()\n",
        "phi_j_train.backward()\n",
        "print(net.weight.grad)\n",
        "print(phi_j_train)\n",
        "print(meta_loss.grad_fn(phi_j_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM_3frhpU0Yc",
        "outputId": "2fed57ef-107a-48a1-ac4f-5ee7ccfdf388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9600, grad_fn=<SumBackward0>)\n",
            "tensor([-1.8000])\n",
            "tensor([0.6000], grad_fn=<SubBackward0>)\n",
            "tensor([0.6000], grad_fn=<ExpandBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZV_C7PtNevog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## UNIT TEST ##\n",
        "net = SingleNet()\n",
        "# Output task-specific gradient\n",
        "inner_grad1 = inner_gradient(net.weight, D1, mode='train')\n",
        "inner_grad2 = inner_gradient(net.weight, D2, mode='train')\n",
        "print(f\"{inner_grad1=}\")\n",
        "print(f\"{inner_grad2=}\")\n",
        "print(\"\")\n",
        "\n",
        "# Output task-specific weights\n",
        "w1 = inner_weight(net.weight, D1)\n",
        "w2 = inner_weight(net.weight, D2)\n",
        "print(f\"{w1=}\")\n",
        "print(f\"{w2=}\")\n",
        "print(\"\")\n",
        "\n",
        "# Output meta gradient\n",
        "meta_grad1 = meta_gradient(net.weight, D1)\n",
        "meta_grad2 = meta_gradient(net.weight, D2)\n",
        "print(f\"{meta_grad1}\")\n",
        "print(f\"{meta_grad2}\")\n",
        "print(\"\")\n",
        "\n",
        "# Output new meta weight\n",
        "theta_1 = meta_loss(net.weight, D_all)\n",
        "print(f\"{theta_1=}\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_m2ALHWRHsF",
        "outputId": "525915b2-1506-4617-a654-4db4d38c795f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inner_grad1=tensor([4.], grad_fn=<SumBackward1>)\n",
            "inner_grad2=tensor([92.], grad_fn=<SumBackward1>)\n",
            "\n",
            "w1=tensor([0.6000], grad_fn=<SubBackward0>)\n",
            "w2=tensor([-8.2000], grad_fn=<SubBackward0>)\n",
            "\n",
            "tensor([-2.2400])\n",
            "tensor([594.8800])\n",
            "\n",
            "theta_1=tensor([-295.3200], grad_fn=<SubBackward0>)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}